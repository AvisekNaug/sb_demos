{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom policy for TD3 to include different actor critic architecture: MyFeedForwardPolicy\n",
    "\n",
    "* CustomFFPTD3 will inherit from TD3Policy class\n",
    "* The custom policy \"CustomFFPTD3 shall form the substitute for \"FeedForwardPolicy\" which inherits from TD3Policy class too\n",
    "* Any Custom TD3Policy will inherit from this newly created \"CustomFFPTD3\" class\n",
    "* It will be based on the more flexible class FeedForwardPolicy(ActorCriticPolicy) https://stable-baselines.readthedocs.io/en/master/_modules/stable_baselines/common/policies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gym.spaces import Box\n",
    "\n",
    "from stable_baselines.td3.policies import TD3Policy\n",
    "from stable_baselines.sac.policies import mlp\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFFPTD3(TD3Policy):\n",
    "    \"\"\"\n",
    "    Policy object that implements a DDPG-like actor critic, using a feed forward neural network. It is only different\n",
    "    from the existing FeedForwardPolicy for TD3 in the way network architectures are defined: here we can define \n",
    "    separate architectures for actor and critic networks.\n",
    "\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param net_arch: (dict) The architecture e of the actor and critic network for the policy (if None, default to [64, 64])\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param layer_norm: (bool) enable layer normalisation\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=1, n_batch=None, reuse=False, net_arch=None,\n",
    "                 cnn_extractor=nature_cnn, feature_extraction=\"cnn\",\n",
    "                 layer_norm=False, act_fun=tf.nn.relu, **kwargs):\n",
    "        super(CustomFFPTD3, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                                                reuse=reuse, scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "        self.layer_norm = layer_norm\n",
    "        self.feature_extraction = feature_extraction\n",
    "        self.cnn_kwargs = kwargs\n",
    "        self.cnn_extractor = cnn_extractor\n",
    "        self.reuse = reuse\n",
    "        if net_arch is None:\n",
    "            net_arch = dict(pi=[64, 64],vf=[64,64])\n",
    "        self.net_arch = net_arch\n",
    "\n",
    "        assert ('pi' in self.net_arch.keys()) & ('vf' in self.net_arch.keys()), \"KeyError: 'pi' and 'vf' keywords missing\"\n",
    "        assert len(self.net_arch['pi']) >= 1, \"Error: must have at least one hidden layer for the actor network.\"\n",
    "        assert len(self.net_arch['vf']) >= 1, \"Error: must have at least one hidden layer for the critics network.\"\n",
    "\n",
    "        self.activ_fn = act_fun\n",
    "\n",
    "    def make_actor(self, obs=None, reuse=False, scope=\"pi\"):\n",
    "        if obs is None:\n",
    "            obs = self.processed_obs\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            if self.feature_extraction == \"cnn\":\n",
    "                pi_h = self.cnn_extractor(obs, **self.cnn_kwargs)\n",
    "            else:\n",
    "                pi_h = tf.layers.flatten(obs)\n",
    "\n",
    "            pi_h = mlp(pi_h, self.net_arch['pi'], self.activ_fn, layer_norm=self.layer_norm)\n",
    "\n",
    "            self.policy = policy = tf.layers.dense(pi_h, self.ac_space.shape[0], activation=tf.tanh)\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def make_critics(self, obs=None, action=None, reuse=False, scope=\"values_fn\"):\n",
    "        if obs is None:\n",
    "            obs = self.processed_obs\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            if self.feature_extraction == \"cnn\":\n",
    "                critics_h = self.cnn_extractor(obs, **self.cnn_kwargs)\n",
    "            else:\n",
    "                critics_h = tf.layers.flatten(obs)\n",
    "\n",
    "            # Concatenate preprocessed state and action\n",
    "            qf_h = tf.concat([critics_h, action], axis=-1)\n",
    "\n",
    "            # Double Q values to reduce overestimation\n",
    "            with tf.variable_scope('qf1', reuse=reuse):\n",
    "                qf1_h = mlp(qf_h, self.net_arch['vf'], self.activ_fn, layer_norm=self.layer_norm)\n",
    "                qf1 = tf.layers.dense(qf1_h, 1, name=\"qf1\")\n",
    "\n",
    "            with tf.variable_scope('qf2', reuse=reuse):\n",
    "                qf2_h = mlp(qf_h, self.net_arch['vf'], self.activ_fn, layer_norm=self.layer_norm)\n",
    "                qf2 = tf.layers.dense(qf2_h, 1, name=\"qf2\")\n",
    "\n",
    "            self.qf1 = qf1\n",
    "            self.qf2 = qf2\n",
    "\n",
    "        return self.qf1, self.qf2\n",
    "\n",
    "    def step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the different policy in TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines import TD3\n",
    "from stable_baselines.td3.policies import FeedForwardPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MLP policy with two layers\n",
    "class CustomTD3Policy(CustomFFPTD3):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomTD3Policy, self).__init__(*args, **kwargs,\n",
    "                                           net_arch = dict(pi=[16, 16],vf=[32,32]),\n",
    "                                           layer_norm=False,\n",
    "                                           feature_extraction=\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and wrap the environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = TD3(CustomTD3Policy, env, action_noise=action_noise, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SmartBuildings]",
   "language": "python",
   "name": "conda-env-SmartBuildings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
