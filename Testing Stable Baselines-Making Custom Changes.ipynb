{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO2(MlpPolicy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom policy for TD3 to include different actor critic architecture: MyFeedForwardPolicy\n",
    "\n",
    "* CustomFFPTD3 will inherit from TD3Policy class\n",
    "* The custom policy \"CustomFFPTD3 shall form the substitute for \"FeedForwardPolicy\" which inherits from TD3Policy class too\n",
    "* Any Custom TD3Policy will inherit from this newly created \"CustomFFPTD3\" class\n",
    "* It will be based on the more flexible class FeedForwardPolicy(ActorCriticPolicy) https://stable-baselines.readthedocs.io/en/master/_modules/stable_baselines/common/policies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gym.spaces import Box\n",
    "\n",
    "from stable_baselines.td3.policies import TD3Policy\n",
    "from stable_baselines.sac.policies import mlp\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFFPTD3(TD3Policy):\n",
    "    \"\"\"\n",
    "    Policy object that implements a DDPG-like actor critic, using a feed forward neural network. It is only different\n",
    "    from the existing FeedForwardPolicy for TD3 in the way network architectures are defined: here we can define \n",
    "    separate architectures for actor and critic networks.\n",
    "\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param net_arch: (dict) The architecture e of the actor and critic network for the policy (if None, default to [64, 64])\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param layer_norm: (bool) enable layer normalisation\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=1, n_batch=None, reuse=False, net_arch=None,\n",
    "                 cnn_extractor=nature_cnn, feature_extraction=\"cnn\",\n",
    "                 layer_norm=False, act_fun=tf.nn.relu, **kwargs):\n",
    "        super(CustomFFPTD3, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                                                reuse=reuse, scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "        self.layer_norm = layer_norm\n",
    "        self.feature_extraction = feature_extraction\n",
    "        self.cnn_kwargs = kwargs\n",
    "        self.cnn_extractor = cnn_extractor\n",
    "        self.reuse = reuse\n",
    "        if net_arch is None:\n",
    "            net_arch = dict(pi=[64, 64],vf=[64,64])\n",
    "        self.net_arch = net_arch\n",
    "\n",
    "        assert ('pi' in self.net_arch.keys()) & ('vf' in self.net_arch.keys()), \"KeyError: 'pi' and 'vf' keywords missing\"\n",
    "        assert len(self.net_arch['pi']) >= 1, \"Error: must have at least one hidden layer for the actor network.\"\n",
    "        assert len(self.net_arch['vf']) >= 1, \"Error: must have at least one hidden layer for the critics network.\"\n",
    "\n",
    "        self.activ_fn = act_fun\n",
    "\n",
    "    def make_actor(self, obs=None, reuse=False, scope=\"pi\"):\n",
    "        if obs is None:\n",
    "            obs = self.processed_obs\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            if self.feature_extraction == \"cnn\":\n",
    "                pi_h = self.cnn_extractor(obs, **self.cnn_kwargs)\n",
    "            else:\n",
    "                pi_h = tf.layers.flatten(obs)\n",
    "\n",
    "            pi_h = mlp(pi_h, self.net_arch['pi'], self.activ_fn, layer_norm=self.layer_norm)\n",
    "\n",
    "            self.policy = policy = tf.layers.dense(pi_h, self.ac_space.shape[0], activation=tf.tanh)\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def make_critics(self, obs=None, action=None, reuse=False, scope=\"values_fn\"):\n",
    "        if obs is None:\n",
    "            obs = self.processed_obs\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            if self.feature_extraction == \"cnn\":\n",
    "                critics_h = self.cnn_extractor(obs, **self.cnn_kwargs)\n",
    "            else:\n",
    "                critics_h = tf.layers.flatten(obs)\n",
    "\n",
    "            # Concatenate preprocessed state and action\n",
    "            qf_h = tf.concat([critics_h, action], axis=-1)\n",
    "\n",
    "            # Double Q values to reduce overestimation\n",
    "            with tf.variable_scope('qf1', reuse=reuse):\n",
    "                qf1_h = mlp(qf_h, self.net_arch['vf'], self.activ_fn, layer_norm=self.layer_norm)\n",
    "                qf1 = tf.layers.dense(qf1_h, 1, name=\"qf1\")\n",
    "\n",
    "            with tf.variable_scope('qf2', reuse=reuse):\n",
    "                qf2_h = mlp(qf_h, self.net_arch['vf'], self.activ_fn, layer_norm=self.layer_norm)\n",
    "                qf2 = tf.layers.dense(qf2_h, 1, name=\"qf2\")\n",
    "\n",
    "            self.qf1 = qf1\n",
    "            self.qf2 = qf2\n",
    "\n",
    "        return self.qf1, self.qf2\n",
    "\n",
    "    def step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the different policy in TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines import TD3\n",
    "from stable_baselines.td3.policies import FeedForwardPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MLP policy with two layers\n",
    "class CustomTD3Policy(CustomFFPTD3):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomTD3Policy, self).__init__(*args, **kwargs,\n",
    "                                           net_arch = dict(pi=[16, 16],vf=[32,32]),\n",
    "                                           layer_norm=False,\n",
    "                                           feature_extraction=\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and wrap the environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = TD3(CustomTD3Policy, env, action_noise=action_noise, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to log different parameters in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines import results_plotter\n",
    "\n",
    "\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\"\n",
    "    global n_steps, best_mean_reward\n",
    "    # Print stats every 1000 calls\n",
    "    if (n_steps + 1) % 1000 == 0:\n",
    "        # Evaluate policy training performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "    n_steps += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\n",
    "# Train the agent\n",
    "time_steps = 10000\n",
    "model.learn(total_timesteps=int(time_steps), callback=callback)\n",
    "\n",
    "results_plotter.plot_results([log_dir], time_steps, results_plotter.X_TIMESTEPS, 'CartPole-v1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./a2c_cartpole_tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging additional trainable vars of my choice on docs example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines import SAC\n",
    "\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v0\", tensorboard_log=\"./sac/\", verbose=1)\n",
    "# Define a new property to avoid global variable\n",
    "model.is_tb_set = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter_list()  # get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameter_list()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_parameters()['target/values_fn/vf/fc0/kernel:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(locals_, globals_):\n",
    "    self_ = locals_['self']\n",
    "    # Log additional tensor\n",
    "    if not self_.is_tb_set:\n",
    "        with self_.graph.as_default():\n",
    "            tf.summary.scalar('value_target', tf.reduce_mean(self_.value_target))\n",
    "            tf.summary.histogram('target/values_fn/vf/fc0/kernel:0',\n",
    "                                 self_.get_parameters()['target/values_fn/vf/fc0/kernel:0'])\n",
    "            self_.summary = tf.summary.merge_all()\n",
    "        self_.is_tb_set = True\n",
    "    # Log scalar value (here a random variable)\n",
    "    value = np.random.random()\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag='random_value', simple_value=value)])\n",
    "    locals_['writer'].add_summary(summary, self_.num_timesteps)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(10000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SmartBuildings]",
   "language": "python",
   "name": "conda-env-SmartBuildings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
